#!/usr/bin/env python3

import argparse
import json
from pathlib import Path


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Evaluate simulated dialogs generated by simulate_llm_dialog.py."
    )
    parser.add_argument(
        "--input",
        type=Path,
        default=Path("outputs/simulated_dialogs.jsonl"),
        help="Path to the JSONL file containing simulated dialogs.",
    )
    parser.add_argument(
        "--success-user-acts",
        type=str,
        default="donate,agree-donation,positive reaction,provide-donation-amount",
        help="Comma-separated user dialog acts considered a donation agreement.",
    )
    return parser.parse_args()


def detect_acceptance(user_da: str, valid_das: set[str]) -> bool:
    return user_da.lower().strip() in valid_das


def evaluate_simulations(path: Path, success_acts: set[str]) -> dict:
	total_dialogs = 0
	success_count = 0
	total_turns_all = 0
	success_turns_all = 0

	with path.open("r", encoding="utf-8") as f:
		for line in f:
			line = line.strip()
			if not line:
				continue
			record = json.loads(line)
			total_dialogs += 1

			turns = record.get("turns", [])
			dialog_success = False
			for turn in turns:
				user_da = str(turn.get("user_dialog_act", "")).lower().strip()
				total_turns_all += 1
				if detect_acceptance(user_da, success_acts):
					dialog_success = True
					success_turns_all += turn.get("turn", 0)
					break

			if dialog_success:
				success_count += 1

	metrics = {
		"num_dialogs": total_dialogs,
		"success_rate_percent": (success_count / total_dialogs * 100.0) if total_dialogs else 0.0,
		"average_turns_success": (success_turns_all / success_count) if success_count else 0.0,
		"overall_avg_turns": (total_turns_all / total_dialogs) if total_dialogs else 0.0,
	}
	return metrics


def main() -> None:
	args = parse_args()
	if not args.input.exists():
		raise FileNotFoundError(f"Simulation file not found at {args.input}")

	valid_das = {kw.strip().lower() for kw in args.success_user_acts.split(",") if kw.strip()}

	metrics = evaluate_simulations(args.input, valid_das)
	print("=== Simulation Metrics ===")
	for key, value in metrics.items():
		if isinstance(value, float):
			print(f"{key}: {value:.2f}")
		else:
			print(f"{key}: {value}")


if __name__ == "__main__":
	main()
